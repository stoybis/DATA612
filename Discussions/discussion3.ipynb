{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fac167",
   "metadata": {},
   "source": [
    "Recommender systems certainly have the ability to reinforce human bias if not carefully monitored. Thus far we have discussed content based filtering (recommending items that are similar to items you have liked) and collaborative filtering (grouping users together based on similarity and recommending products that the group as a whole likes. These algorithms have the potential for bias reinforcement. For example, in the YouTube Talk “When Recommendation Systems Go Bad” Evan Estola gives an example of how Amazon recommends additional books by Isaac Asimov to users who have purchased an Isaac Asimov book (content based filtering). This is relatively harmless, but does reinforce the user’s bias to Asimov and does not recommend other authors or genres that they could actually be interest in. A more harmful example is how MeetUp groups that focused on tech would not be recommended to women because women historically have made up a small percentage of these meetups. This is a harmful example as it excludes a group based on their gender (a protected characteristic) and reinforces bias in the meetups if not corrected (if women are not shown tech meetups, they will be unlikely to attend and the bias continues). The MeetUp team had to correct this by separating interests and gender among different algorithms and used ensemble methods to create recommendations, meaning that tech meetups were not suggested purely based on gender.\n",
    "\n",
    "Another interesting example that has real life implications is recommendation engines on dating apps. Recommending certain profiles to users have real life implications as the ultimate goal for users is to meet in real life and a poor match can lead to a bad date (or worse) while a good match can potentially lead to a successful marriage. The article “Finding Love on a First Data: Matching Algorithms in Online Dating” discusses the evolution of online dating services/apps on the approach they take to recommendations. The article mentions how the use of collaborative filtering in dating apps (grouping users based on similarity and recommending profiles) has the potential for racial bias to creep in and exclude a group of users from seeing users of a different race and making dating apps exclusionary rather than inclusive. One way that developers controlled for this is by allowing users to delete their history to reset the algorithm or to opt out of the algorithm entirely. This reminds me of our class discussions about balancing minimizing an error metric in a recommendation algorithm with allowing some error in order to not put a user in a certain box so that they can receive recommendations outside of their immediate interests that the user might actually find interesting. \n",
    "\n",
    "Lastly, a book that I have on my “to-read” list discusses this topic: “Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy” by Cathy O’Neil\n",
    "\n",
    "Links:\n",
    "https://www.youtube.com/watch?v=MqoRzNhrTnQ\n",
    "https://hdsr.mitpress.mit.edu/pub/i4eb4e8b/release/3\n",
    "https://www.goodreads.com/book/show/28186015-weapons-of-math-destruction?ac=1&from_search=true&qid=a0cogZvoCd&rank=1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
