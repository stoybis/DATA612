{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e40f77-ffd2-4afd-928c-26b8c83e5f97",
   "metadata": {},
   "source": [
    "# Research Discussion 2 - Semyon Toybis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7132fa-b142-4bed-a456-d06044f06100",
   "metadata": {},
   "source": [
    "The YouTube video “Music Recommendations at Scale with Spark – Christopher Johnson (Spotify)” discusses Spotify’s approach to implementing a collaborative filtering algorithm to recommend tracks or artists to users. \n",
    "\n",
    "Christopher Johnson mentioned that the problem was like the Netflix recommendation problem; however instead of ratings for movies, Spotify had a user-item matrix where 1 represented whether a user listened to a track and 0 if the user did not. Spotify inferred that if a user listened to a track, they liked the track, and they implemented a weighting technique where the more times a user listened to a track, the more influence that would have in their algorithm. The Spotify team used matrix decomposition where they tried to estimate the user-item matrix by breaking it down into a user vector and a song vector, fixed one of the vectors and solved for the other (alternating least squares), seeking to minimize the cost function (RMSE). \n",
    "\n",
    "Christopher Johnson then discussed how Spotify implemented this approach via Hadoop and Spark. In order to implement the ALS algorithm for matrix decomposition, Spotify had to use parallel computing where it delegated different blocks of the data to different nodes. While the team initially used Hadoop, the found that using Spark resulted in a significant improvement in computation time: Hadoop required ten hours of run time vs 3.5 hours with Spark with full gridify and 1.5 hours for Spark with half gridify for a dataset of 4 million users and 500k artists.\n",
    "\n",
    "The key difference was that Hadoop required continual reading and writing of the dataset whereas Spark loaded the user-item matrix into memory and didn’t have to re-read it every time. As someone who is new to Hadoop and Spark, I found the below articles helpful in understanding the differences:\n",
    "\n",
    "https://www.datacamp.com/blog/hadoop-vs-spark\n",
    "\n",
    "https://www.ibm.com/think/insights/hadoop-vs-spark\n",
    "\n",
    "As mentioned in the Spotify video, eliminating the need to constantly read and write the dataset in Spark improves the run time vs Hadoop. My understanding is that the more times a dataset needs to be iterated over and the more “real-time” the results have to be result in Spark being a more appropriate framework. However, Spark does seem to be more expensive as it requires more RAM though Hadoop can be expensive as well due to the large need for disk space.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
